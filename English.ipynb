{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/16217985608432610\n"
     ]
    }
   ],
   "source": [
    "# Code based of https://dylancope.github.io/Multiagent-RL-with-TFAgents/\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "import random\n",
    "from time import time\n",
    "from typing import Tuple, List, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_agents.typing import types\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import DqnAgent\n",
    "from tf_agents.agents.tf_agent import LossInfo\n",
    "from tf_agents.environments.py_environment import PyEnvironment\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.specs import TensorSpec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n\\n')\n",
    "\n",
    "OUTPUTS_DIR = f'./outputs/{int(10000000 * time())}'\n",
    "print('Output Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "LOST = 0.95\n",
    "DISCOUNT = np.float32(1.0)\n",
    "import abc\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "class EnglishAuction(py_environment.PyEnvironment):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        bid_spec = BoundedArraySpec(\n",
    "            (), dtype=np.int32, minimum=0, maximum=2\n",
    "        )\n",
    "        player_spec = BoundedArraySpec(shape=(1,),dtype=np.int32, minimum=1, maximum=1)\n",
    "        self._action_spec = {\n",
    "            \"bid\": bid_spec,\n",
    "            \"player\": player_spec\n",
    "        }\n",
    "        minimum = np.zeros((1,5), dtype=np.int32)\n",
    "        maximum = np.array([[100,2,2,100,100]], dtype=np.int32)\n",
    "        self._observation_spec = BoundedArraySpec(\n",
    "            shape=(1,5), dtype=np.int32, name=\"observation\", maximum=maximum, minimum=minimum\n",
    "        )\n",
    "        self._state = np.zeros(shape=(1,7), dtype=np.int32)\n",
    "        self._state[0,1:3] = 2\n",
    "        valuations = np.zeros((1,4))\n",
    "        for x in range(2):\n",
    "            valu = np.random.rand(1,2)\n",
    "            valuations[0,2*x] = np.amax(valu) * 100\n",
    "            valuations[0,1+2*x] = np.amin(valu) * 100\n",
    "\n",
    "        self._state[:,3:7] = valuations\n",
    "        self._done = False\n",
    "        self._current_time_step = TimeStep(step_type=StepType.FIRST, reward=0, discount=DISCOUNT, observation=self._state)\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.zeros(shape=(1,7), dtype=np.int32)\n",
    "        self._state[0,1:3] = 2\n",
    "        valuations = np.zeros((1,4))\n",
    "        for x in range(2):\n",
    "            valu = np.random.rand(1,2)\n",
    "            valuations[0,2*x] = np.amax(valu) * 100\n",
    "            valuations[0,1+2*x] = np.amin(valu) * 100\n",
    "\n",
    "        self._state[:,3:7] = valuations\n",
    "        self._done = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def __calculateWinnings(self, player):\n",
    "        player_payment = self._state[0, 0] * self._state[0,player]\n",
    "        player_revenue = 0\n",
    "        if self._state[0,player] >= 1:\n",
    "            player_revenue += self._state[0, 3 + (player-1)*2]\n",
    "        if self._state[0,player] == 2:\n",
    "            player_revenue += self._state[0, 4 + (player-1)*2]\n",
    "        return np.float32(player_revenue - player_payment)\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        if self._done:\n",
    "            return self._reset()\n",
    "\n",
    "        player = action[\"player\"]\n",
    "        bid = action[\"bid\"]\n",
    "        if self._state[0, 1] + self._state[0, 2] <= 2:\n",
    "            return TimeStep(StepType.LAST,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "        self._state[0,player] = bid\n",
    "        if self._state[0, 1] + self._state[0, 2] <= 2:\n",
    "            return TimeStep(StepType.MID,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "        res_rev = np.float32(0)\n",
    "        if self.__calculateWinnings(player) < -20:\n",
    "            res_rev += self.__calculateWinnings(player)\n",
    "        if res_rev < -150:\n",
    "            return TimeStep(\n",
    "            StepType.LAST,\n",
    "            res_rev * np.float32(3),\n",
    "            DISCOUNT,\n",
    "            self._state)\n",
    "\n",
    "        self._state[0,0] += 1\n",
    "        res_rev *= np.float32(0.1)\n",
    "        return TimeStep(\n",
    "            StepType.MID,\n",
    "            res_rev,\n",
    "            DISCOUNT,\n",
    "            self._state\n",
    "        )\n",
    "    def observation_spec(self) -> types.NestedArraySpec:\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self) -> types.NestedArraySpec:\n",
    "        return self._action_spec\n",
    "\n",
    "    def get_info(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    def get_state(self) -> Any:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, state: Any) -> None:\n",
    "        self._state = state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class IMAgent(DqnAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: TFPyEnvironment,\n",
    "                 player_id,\n",
    "                 observation_spec: TensorSpec = None,\n",
    "                 action_spec: TensorSpec = None,\n",
    "                 reward_fn: Callable = lambda time_step: time_step.reward,\n",
    "                 action_fn: Callable = lambda action: action,\n",
    "                 name: str='IMAgent',\n",
    "                 q_network=None,\n",
    "                 # training params\n",
    "                 replay_buffer_max_length: int = 1000,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 training_batch_size: int = 8,\n",
    "                 training_parallel_calls: int = 3,\n",
    "                 training_prefetch_buffer_size: int = 3,\n",
    "                 training_num_steps: int = 2,\n",
    "                 **dqn_kwargs):\n",
    "\n",
    "        self._env = env\n",
    "        self._reward_fn = reward_fn\n",
    "        self._name = name\n",
    "        self._observation_spec = observation_spec or self._env.observation_spec()\n",
    "        self._action_spec = action_spec or self._env.action_spec()\n",
    "        self._action_fn = action_fn\n",
    "        self.player_id = player_id\n",
    "\n",
    "        self.q_network = q_network or self._build_q_net()\n",
    "\n",
    "        env_ts_spec = self._env.time_step_spec()\n",
    "        time_step_spec = TimeStep(\n",
    "            step_type=env_ts_spec.step_type,\n",
    "            reward=env_ts_spec.reward,\n",
    "            discount=env_ts_spec.discount,\n",
    "            observation=self.q_network.input_tensor_spec\n",
    "        )\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        super().__init__(time_step_spec,\n",
    "                         self._action_spec,\n",
    "                         self.q_network,\n",
    "                         optimizer,\n",
    "                         name=name,\n",
    "                         epsilon_greedy=0.05,\n",
    "                         **dqn_kwargs)\n",
    "\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size)\n",
    "        self._rewards = []\n",
    "\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=self.collect_data_spec,\n",
    "            batch_size=self._env.batch_size,\n",
    "            max_length=replay_buffer_max_length)\n",
    "\n",
    "        self._training_batch_size = training_batch_size\n",
    "        self._training_parallel_calls = training_parallel_calls\n",
    "        self._training_prefetch_buffer_size = training_prefetch_buffer_size\n",
    "        self._training_num_steps = training_num_steps\n",
    "        self.train = common.function(self.train)\n",
    "\n",
    "    def _build_q_net(self):\n",
    "#         q_net = QRnnNetwork(input_tensor_spec=self._observation_spec,\n",
    "#                             action_spec=self._action_spec,\n",
    "#                             name=f'{self._name}QRNN')\n",
    "\n",
    "        fc_layer_params = (64, 64, 64, 32)\n",
    "        q_net = QNetwork(\n",
    "            self._observation_spec,\n",
    "            self._action_spec,\n",
    "            fc_layer_params=fc_layer_params,\n",
    "            activation_fn=tf.nn.leaky_relu\n",
    "        )\n",
    "\n",
    "        q_net.create_variables()\n",
    "        q_net.summary()\n",
    "\n",
    "        return q_net\n",
    "\n",
    "    def reset(self):\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size\n",
    "        )\n",
    "        self._rewards = []\n",
    "\n",
    "    def episode_return(self) -> float:\n",
    "        return np.sum(self._rewards)\n",
    "\n",
    "    def _observation_fn(self, observation: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        my_id = self.player_id\n",
    "        enemy_id = 2 if my_id == 1 else 1\n",
    "\n",
    "        price = tf.slice(observation, [0,0,0],[1,1,1])\n",
    "        my_demand = tf.slice(observation, [0,0,my_id],[1,1,1])\n",
    "        enemy_demand = tf.slice(observation, [0,0,enemy_id],[1,1, 1])\n",
    "\n",
    "        correct_column_index = 3 + 2 * (my_id- 1)\n",
    "        my_values = observation[:,:,correct_column_index:correct_column_index+2]\n",
    "        res = tf.concat([price, my_demand, enemy_demand, my_values],2)\n",
    "        return res\n",
    "\n",
    "    def _augment_time_step(self, time_step: TimeStep) -> TimeStep:\n",
    "\n",
    "        reward = self._reward_fn(time_step)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        if reward.shape != time_step.reward.shape:\n",
    "            reward = tf.reshape(reward, time_step.reward.shape)\n",
    "\n",
    "        observation = self._observation_fn(time_step.observation)\n",
    "\n",
    "        return TimeStep(\n",
    "            step_type=time_step.step_type,\n",
    "            reward=reward,\n",
    "            discount=time_step.discount,\n",
    "            observation=observation\n",
    "        )\n",
    "\n",
    "    def _current_time_step(self) -> TimeStep:\n",
    "        time_step = self._env.current_time_step()\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def _step_environment(self, action) -> TimeStep:\n",
    "        action = self._action_fn(action)\n",
    "        time_step = self._env.step(action)\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def act(self, collect=False) -> Trajectory:\n",
    "        time_step = self._current_time_step()\n",
    "\n",
    "        if collect:\n",
    "            policy_step = self.collect_policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "        else:\n",
    "            policy_step = self.policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "\n",
    "        self._policy_state = policy_step.state\n",
    "        next_time_step = self._step_environment(policy_step.action)\n",
    "        traj = trajectory.from_transition(time_step, policy_step, next_time_step)\n",
    "\n",
    "        self._rewards.append(next_time_step.reward)\n",
    "\n",
    "        if collect:\n",
    "            self._replay_buffer.add_batch(traj)\n",
    "\n",
    "        return traj\n",
    "\n",
    "    def train_iteration(self) -> LossInfo:\n",
    "        experience, info = self._replay_buffer.get_next(\n",
    "            sample_batch_size=self._training_batch_size,\n",
    "            num_steps=self._training_num_steps\n",
    "        )\n",
    "        return self.train(experience)\n",
    "\n",
    "    def getQNetwork(self):\n",
    "        return self.q_network\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "def training_episode(env, player_nr_1, player_nr_2):\n",
    "    time_step = env.reset()\n",
    "    player_nr_1.reset()\n",
    "    player_nr_2.reset()\n",
    "    time_steps = []\n",
    "    players = cycle([player_nr_1, player_nr_2])\n",
    "    while not time_step.is_last():\n",
    "        player = next(players)\n",
    "        player.act(collect=True)\n",
    "        time_step = env.current_time_step()\n",
    "        time_steps.append(time_step)\n",
    "    return time_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def collect_training_data():\n",
    "    for game in range(episodes_per_iteration):\n",
    "        final_timestep = training_episode(auction, player_1, player_2)\n",
    "        obsi = final_timestep[-1].observation.numpy()\n",
    "        valuations = obsi[:,:,3:]\n",
    "        truthful_price = np.sort(valuations)[0,0,1]\n",
    "\n",
    "        p1_value_gained = 0\n",
    "        if truthful_price < obsi[0,0,3]:\n",
    "            p1_value_gained += obsi[0,0,3]\n",
    "\n",
    "        if truthful_price < obsi[0,0,4]:\n",
    "            p1_value_gained += obsi[0,0,4]\n",
    "\n",
    "        p2_value_gained = 0\n",
    "        if truthful_price < obsi[0,0,5]:\n",
    "            p2_value_gained += obsi[0,0,5]\n",
    "\n",
    "        if truthful_price < obsi[0,0,6]:\n",
    "            p2_value_gained += obsi[0,0,6]\n",
    "\n",
    "        p1_payments = truthful_price * obsi[0,0,1]\n",
    "        p2_payments = truthful_price * obsi[0,0,2]\n",
    "\n",
    "        p1_truthful_revenue = p1_value_gained - p1_payments\n",
    "        p2_truthful_revenue = p2_value_gained - p2_payments\n",
    "\n",
    "        player = 1\n",
    "        player_1_played_payment = obsi[0,0, 0] * obsi[0,0,player]\n",
    "        player_1_played_revenue = 0\n",
    "        if obsi[0,0,player] >= 1:\n",
    "            player_1_played_revenue += obsi[0,0, 3 + (player-1)*2]\n",
    "        if obsi[0,0,player] == 2:\n",
    "            player_1_played_revenue += obsi[0,0, 4 + (player-1)*2]\n",
    "        p1_played_revenue = player_1_played_revenue - player_1_played_payment\n",
    "\n",
    "        player = 2\n",
    "        player_2_played_payment = obsi[0,0, 0] * obsi[0,0,player]\n",
    "        player_2_played_revenue = 0\n",
    "        if obsi[0,0,player] >= 1:\n",
    "            player_2_played_revenue += obsi[0,0, 3 + (player-1)*2]\n",
    "        if obsi[0,0,player] == 2:\n",
    "            player_2_played_revenue += obsi[0,0, 4 + (player-1)*2]\n",
    "        p2_played_revenue = player_2_played_revenue - player_2_played_payment\n",
    "\n",
    "\n",
    "        p1_performance = p1_played_revenue - p1_truthful_revenue\n",
    "        p2_performance = p2_played_revenue - p2_truthful_revenue\n",
    "\n",
    "        p1_return = player_1.episode_return()\n",
    "        p2_return = player_2.episode_return()\n",
    "        outcome = \"draw\"\n",
    "        if p1_return > p2_return:\n",
    "            outcome = \"p1_win\"\n",
    "        elif p1_return < p2_return:\n",
    "            outcome = \"p2_win\"\n",
    "        games.append({\n",
    "            'iteration': iteration,\n",
    "            'game': game,\n",
    "            'p1_return': p1_return,\n",
    "            'p2_return': p2_return,\n",
    "            'outcome': outcome,\n",
    "            'final_step': auction.current_time_step(),\n",
    "            'p1_performance': p1_performance,\n",
    "            'p2_performance': p2_performance,\n",
    "        })\n",
    "\n",
    "def train():\n",
    "    for _ in range (train_steps_per_iteration):\n",
    "        p1_train_info = player_1.train_iteration()\n",
    "        p2_train_info = player_2.train_iteration()\n",
    "\n",
    "        loss_infos.append({\n",
    "            'iteration': iteration,\n",
    "            'p1_loss': p1_train_info.loss.numpy(),\n",
    "            'p2_loss': p2_train_info.loss.numpy(),\n",
    "        })\n",
    "\n",
    "def plot_history():\n",
    "\n",
    "    games_data = pd.DataFrame.from_records(games)\n",
    "    loss_data = pd.DataFrame.from_records(loss_infos)\n",
    "    loss_data['Player 1'] = np.log(loss_data.p1_loss)\n",
    "    loss_data['Player 2'] = np.log(loss_data.p2_loss)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    loss_melted = pd.melt(loss_data,\n",
    "                          id_vars=['iteration'],\n",
    "                          value_vars=['Player 1', 'Player 2'])\n",
    "    smoothing = iteration // 50\n",
    "    loss_melted.iteration = smoothing * (loss_melted.iteration // smoothing)\n",
    "\n",
    "    sns.lineplot(ax=axs[0][0],\n",
    "                 x='iteration', hue='variable',\n",
    "                 y='value', data=loss_melted)\n",
    "    axs[0][0].set_title('Loss History')\n",
    "    axs[0][0].set_ylabel('log-loss')\n",
    "\n",
    "    returns_melted = pd.melt(games_data,\n",
    "                             id_vars=['iteration'],\n",
    "                             value_vars=['p1_return', 'p2_return'])\n",
    "    returns_melted.iteration = smoothing * (returns_melted.iteration // smoothing)\n",
    "    sns.lineplot(ax=axs[0][1],\n",
    "                 x='iteration', hue='variable',\n",
    "                 y='value', data=returns_melted)\n",
    "    axs[0][1].set_title('Return History')\n",
    "    axs[0][1].set_ylabel('return')\n",
    "\n",
    "    performance_melted = pd.melt(games_data,\n",
    "                             id_vars=['iteration'],\n",
    "                             value_vars=['p1_performance', 'p2_performance'])\n",
    "    performance_melted.iteration = smoothing * (performance_melted.iteration // smoothing)\n",
    "    sns.lineplot(ax=axs[1][1],\n",
    "                 x='iteration', hue='variable',\n",
    "                 y='value', data=performance_melted)\n",
    "    axs[1][1].set_title('Performance History')\n",
    "    axs[1][1].set_ylabel('performance')\n",
    "\n",
    "    games_data['p1_win'] = games_data.outcome == 'p1_win'\n",
    "    games_data['p2_win'] = games_data.outcome == 'p2_win'\n",
    "    grouped_games_data = games_data.groupby('iteration')\n",
    "    cols = ['game', 'p1_win', 'p2_win']\n",
    "    grouped_games_data = grouped_games_data[cols]\n",
    "    game_totals =  grouped_games_data.max()['game'] + 1\n",
    "    summed_games_data = grouped_games_data.sum()\n",
    "    summed_games_data['p1_win_rate'] = summed_games_data.p1_win / game_totals\n",
    "    summed_games_data['p2_win_rate'] = summed_games_data.p2_win / game_totals\n",
    "    summed_games_data['iteration'] = smoothing * (summed_games_data.index // smoothing)\n",
    "\n",
    "    sns.lineplot(ax=axs[1][0],\n",
    "                 x='iteration',\n",
    "                 y='p1_win_rate',\n",
    "                 data=summed_games_data,\n",
    "                 label='Player 1 Win Rate')\n",
    "    sns.lineplot(ax=axs[1][0],\n",
    "                 x='iteration',\n",
    "                 y='p2_win_rate',\n",
    "                 data=summed_games_data,\n",
    "                 label='Player 2 Win Rate')\n",
    "    axs[1][0].set_title('Outcomes History')\n",
    "    axs[1][0].set_ylabel('ratio')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "num_iterations = 2000\n",
    "initial_collect_episodes = 75\n",
    "episodes_per_iteration = 50\n",
    "train_steps_per_iteration = 1\n",
    "training_batch_size = 256\n",
    "training_num_steps = 1\n",
    "replay_buffer_size = 10 * episodes_per_iteration\n",
    "learning_rate = 1e-4\n",
    "plot_interval = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "games = []\n",
    "loss_infos = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  10784     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             multiple                  99        \n",
      "=================================================================\n",
      "Total params: 10,883\n",
      "Trainable params: 10,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def ttt_action_fn(player, action):\n",
    "    return {'position': action, 'value': player}\n",
    "\n",
    "auction = EnglishAuction()\n",
    "auction = tf_py_environment.TFPyEnvironment(auction)\n",
    "player_1 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,1),\n",
    "                   name=\"player1\",\n",
    "                   learning_rate=learning_rate,\n",
    "                   training_batch_size=training_batch_size,\n",
    "                   training_num_steps=training_num_steps,\n",
    "                   replay_buffer_max_length=replay_buffer_size,\n",
    "                   td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                   player_id=1\n",
    "                   )\n",
    "\n",
    "player_2 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,2),\n",
    "                   name=\"player2\",\n",
    "                   learning_rate=learning_rate,\n",
    "                   training_batch_size=training_batch_size,\n",
    "                   training_num_steps=training_num_steps,\n",
    "                   replay_buffer_max_length=replay_buffer_size,\n",
    "                   td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                   player_id=2,\n",
    "                   q_network=player_1.getQNetwork()\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Initial Training Sample...\n",
      "Samples collected\n"
     ]
    }
   ],
   "source": [
    "print('Collecting Initial Training Sample...')\n",
    "for _ in range(initial_collect_episodes):\n",
    "    training_episode(auction, player_1, player_2)\n",
    "print('Samples collected')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-2f2c68b6d622>:153: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The agent was configured to expect a `sequence_length` of '2'. Value is expected to be shaped `[B, T] + spec.shape` but at least one of the Tensors in `value` has a time axis dim value '1' vs the expected '2'.\nFirst such tensor is:\n\tvalue.step_type. \nFull shape structure of value:\n\tTrajectory(step_type=TensorShape([256, 1]), observation=TensorShape([256, 1, 1, 5]), action=TensorShape([256, 1]), policy_info=(), next_step_type=TensorShape([256, 1]), reward=TensorShape([256, 1]), discount=TensorShape([256, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-3b24a60827e5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0miteration\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mnum_iterations\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mcollect_training_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0miteration\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0miteration\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mplot_interval\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-14-58e7c723253a>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtrain_steps_per_iteration\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m         \u001B[0mp1_train_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplayer_1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0mp2_train_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplayer_2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-2f2c68b6d622>\u001B[0m in \u001B[0;36mtrain_iteration\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    155\u001B[0m             \u001B[0mnum_steps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_training_num_steps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         )\n\u001B[0;32m--> 157\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperience\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    158\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    159\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgetQNetwork\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 828\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xla\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    869\u001B[0m       \u001B[0;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    870\u001B[0m       \u001B[0minitializers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 871\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minitializers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    872\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    873\u001B[0m       \u001B[0;31m# At this point we know that the initialization is complete (or less\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[0;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[1;32m    723\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_graph_deleter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFunctionDeleter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lifted_initializer_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    724\u001B[0m     self._concrete_stateful_fn = (\n\u001B[0;32m--> 725\u001B[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m    726\u001B[0m             *args, **kwds))\n\u001B[1;32m    727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2967\u001B[0m       \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2968\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2969\u001B[0;31m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2970\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2971\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   3359\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3360\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmissed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_context_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3361\u001B[0;31m           \u001B[0mgraph_function\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3362\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m   3194\u001B[0m     \u001B[0marg_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbase_arg_names\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmissing_arg_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3195\u001B[0m     graph_function = ConcreteFunction(\n\u001B[0;32m-> 3196\u001B[0;31m         func_graph_module.func_graph_from_py_func(\n\u001B[0m\u001B[1;32m   3197\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3198\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_python_function\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[1;32m    988\u001B[0m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moriginal_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpython_func\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    989\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 990\u001B[0;31m       \u001B[0mfunc_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    991\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    992\u001B[0m       \u001B[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m    632\u001B[0m             \u001B[0mxla_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    633\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 634\u001B[0;31m           \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    635\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/tf_agent.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, experience, weights, **kwargs)\u001B[0m\n\u001B[1;32m    516\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    517\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_enable_functions\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 518\u001B[0;31m       loss_info = self._train_fn(\n\u001B[0m\u001B[1;32m    519\u001B[0m           experience=experience, weights=weights, **kwargs)\n\u001B[1;32m    520\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/utils/common.py\u001B[0m in \u001B[0;36mwith_check_resource_vars\u001B[0;34m(*fn_args, **fn_kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m         \u001B[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \u001B[0;31m# autodep-like behavior is already expected of fn.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mfn_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfn_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresource_variables_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMISSING_RESOURCE_VARIABLES_ERROR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/dqn/dqn_agent.py\u001B[0m in \u001B[0;36m_train\u001B[0;34m(self, experience, weights)\u001B[0m\n\u001B[1;32m    384\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexperience\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    385\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 386\u001B[0;31m       loss_info = self._loss(\n\u001B[0m\u001B[1;32m    387\u001B[0m           \u001B[0mexperience\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m           \u001B[0mtd_errors_loss_fn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_td_errors_loss_fn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/dqn/dqn_agent.py\u001B[0m in \u001B[0;36m_loss\u001B[0;34m(self, experience, td_errors_loss_fn, gamma, reward_scale_factor, weights, training)\u001B[0m\n\u001B[1;32m    448\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mnumber\u001B[0m \u001B[0mof\u001B[0m \u001B[0mactions\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mgreater\u001B[0m \u001B[0mthan\u001B[0m \u001B[0;36m1.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    449\u001B[0m     \"\"\"\n\u001B[0;32m--> 450\u001B[0;31m     \u001B[0mtransition\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_as_transition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperience\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    451\u001B[0m     \u001B[0mtime_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpolicy_steps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext_time_steps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransition\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    452\u001B[0m     \u001B[0mactions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpolicy_steps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/data_converter.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m    417\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    418\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrajectory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrajectory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 419\u001B[0;31m       _validate_trajectory(\n\u001B[0m\u001B[1;32m    420\u001B[0m           \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    421\u001B[0m           \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrajectory_spec\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/data_converter.py\u001B[0m in \u001B[0;36m_validate_trajectory\u001B[0;34m(value, trajectory_spec, sequence_length, num_outer_dims)\u001B[0m\n\u001B[1;32m    161\u001B[0m                 \u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                 debug_str=debug_str))\n\u001B[0;32m--> 163\u001B[0;31m     \u001B[0mnest_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure_with_paths\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheck_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36mmap_structure_with_paths\u001B[0;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[1;32m    697\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstring_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 699\u001B[0;31m   return map_structure_with_tuple_paths_up_to(structure[0],\n\u001B[0m\u001B[1;32m    700\u001B[0m                                               \u001B[0mwrapper_func\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m                                               \u001B[0;34m*\u001B[0m\u001B[0mstructure\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36mmap_structure_with_tuple_paths_up_to\u001B[0;34m(shallow_tree, func, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1255\u001B[0m   flat_path_gen = (\n\u001B[1;32m   1256\u001B[0m       path for path, _ in _yield_flat_up_to(shallow_tree, inputs[0], is_seq))\n\u001B[0;32m-> 1257\u001B[0;31m   results = [\n\u001B[0m\u001B[1;32m   1258\u001B[0m       \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0margs\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflat_path_gen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mflat_value_gen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m   ]\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1256\u001B[0m       path for path, _ in _yield_flat_up_to(shallow_tree, inputs[0], is_seq))\n\u001B[1;32m   1257\u001B[0m   results = [\n\u001B[0;32m-> 1258\u001B[0;31m       \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0margs\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflat_path_gen\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mflat_value_gen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1259\u001B[0m   ]\n\u001B[1;32m   1260\u001B[0m   return pack_sequence_as(structure=shallow_tree, flat_sequence=results,\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001B[0m in \u001B[0;36mwrapper_func\u001B[0;34m(tuple_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    695\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mwrapper_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtuple_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    696\u001B[0m     \u001B[0mstring_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtuple_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 697\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstring_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    698\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m   return map_structure_with_tuple_paths_up_to(structure[0],\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tf_agents/agents/data_converter.py\u001B[0m in \u001B[0;36mcheck_shape\u001B[0;34m(path, t)\u001B[0m\n\u001B[1;32m    149\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0msequence_length\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m         \u001B[0mdebug_str\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mtp\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         raise ValueError(\n\u001B[0m\u001B[1;32m    152\u001B[0m             \u001B[0;34m'The agent was configured to expect a `sequence_length` '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m             \u001B[0;34m'of \\'{seq_len}\\'. Value is expected to be shaped `[B, T] + '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The agent was configured to expect a `sequence_length` of '2'. Value is expected to be shaped `[B, T] + spec.shape` but at least one of the Tensors in `value` has a time axis dim value '1' vs the expected '2'.\nFirst such tensor is:\n\tvalue.step_type. \nFull shape structure of value:\n\tTrajectory(step_type=TensorShape([256, 1]), observation=TensorShape([256, 1, 1, 5]), action=TensorShape([256, 1]), policy_info=(), next_step_type=TensorShape([256, 1]), reward=TensorShape([256, 1]), discount=TensorShape([256, 1]))"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if iteration > 1:\n",
    "        plot_history()\n",
    "        clear_output(wait=True)\n",
    "    while iteration < num_iterations:\n",
    "        collect_training_data()\n",
    "        train()\n",
    "        iteration += 1\n",
    "        if iteration % plot_interval == 0:\n",
    "            plot_history()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    clear_output(wait=True)\n",
    "    print('Interrupting training, plotting history...')\n",
    "    plot_history()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(env, player_nr_1, player_nr_2):\n",
    "    time_step = env.reset()\n",
    "    player_nr_1.reset()\n",
    "    player_nr_2.reset()\n",
    "    players = cycle([player_nr_1, player_nr_2])\n",
    "    while True:\n",
    "        player = next(players)\n",
    "        player.act(collect=False)\n",
    "        if env.current_time_step().is_last():\n",
    "            return env.current_time_step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    last_timestep = test(auction, player_1, player_2)\n",
    "    obs = last_timestep.observation.numpy()\n",
    "\n",
    "    player = 1\n",
    "    payment = obs[0,0, 0] * obs[0,0,player]\n",
    "    revenue = 0\n",
    "    if obs[0,0,player] >= 1:\n",
    "        revenue += obs[0,0, 3 + (player-1)*2]\n",
    "    if obs[0,0,player] == 2:\n",
    "        revenue += obs[0,0, 4 + (player-1)*2]\n",
    "\n",
    "    player_1_revenue = revenue - payment\n",
    "\n",
    "    player = 2\n",
    "    payment = obs[0,0, 0] * obs[0,0,player]\n",
    "    revenue = 0\n",
    "    if obs[0,0,player] >= 1:\n",
    "        revenue += obs[0,0, 3 + (player-1)*2]\n",
    "    if obs[0,0,player] == 2:\n",
    "        revenue += obs[0,0, 4 + (player-1)*2]\n",
    "\n",
    "    player_2_revenue = revenue - payment\n",
    "\n",
    "    player_1_items = obs[0,0,1]\n",
    "    player_2_items = obs[0,0,2]\n",
    "    print(f\"Player 1 won {player_1_items} with revenue {player_1_revenue}\")\n",
    "    print(f\"Player 2 won {player_2_items} with revenue {player_2_revenue}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}