{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/16205694402578152\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "import random\n",
    "from time import time\n",
    "from typing import Tuple, List, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_agents.typing import types\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import DqnAgent\n",
    "from tf_agents.agents.tf_agent import LossInfo\n",
    "from tf_agents.environments.py_environment import PyEnvironment\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.specs import TensorSpec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n\\n')\n",
    "\n",
    "OUTPUTS_DIR = f'./outputs/{int(10000000 * time())}'\n",
    "print('Output Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "LOST = 0\n",
    "DISCOUNT = 1\n",
    "import abc\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "class EnglishAuction(py_environment.PyEnvironment):\n",
    "\n",
    "\n",
    "    def __init__(self, valuations):\n",
    "        bid_spec = BoundedArraySpec(\n",
    "            (), dtype=np.int32, minimum=0, maximum=4\n",
    "        )\n",
    "        player_spec = BoundedArraySpec(shape=(1,),dtype=np.int32, minimum=1, maximum=1)\n",
    "        self._action_spec = {\n",
    "            \"bid\": bid_spec,\n",
    "            \"player\": player_spec\n",
    "        }\n",
    "        minimum = np.zeros((2,5), dtype=np.int32)\n",
    "        maximum = np.array([[1000,2,1,1,1000],[1000,2,1,1,1000]], dtype=np.int32)\n",
    "        self._observation_spec = BoundedArraySpec(\n",
    "            shape=(2,5), dtype=np.int32, name=\"observation\", maximum=maximum, minimum=minimum\n",
    "        )\n",
    "        # highest bid, winning player, p1 active, p2 active, p1 valuation, p2 valuation\n",
    "        self._state = np.zeros(shape=(2,6), dtype=np.int32)\n",
    "        self._state[:,4:6] = valuations\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.zeros(shape=(2,6), dtype=np.int32)\n",
    "         #gen valuations\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def __calculateWinnings(self, player):\n",
    "        item_1 = (self._state[0,1] == player) * (self._state[0, 3 + player] - self._state[0,0])\n",
    "        item_2 = (self._state[1,1] == player) * (self._state[1, 3 + player] - self._state[1,0])\n",
    "        return item_1 + item_2\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._current_time_step.is_last():\n",
    "            return self._reset()\n",
    "\n",
    "        player = action[\"player\"]\n",
    "        bid = action[\"bid\"]\n",
    "\n",
    "        if np.all((self._state[0:1, 2:3] == 0)):\n",
    "            return TimeStep(StepType.LAST,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "\n",
    "        for i in range(2):\n",
    "            if bid == 0:                        # Pass both\n",
    "                self._state[0,1+player] = 0\n",
    "                self._state[1,1+player] = 0\n",
    "            if bid == 1:                        # Increment on Item_1, stay on Item_2\n",
    "                self._state[0,1+player] = 1\n",
    "                self._state[1,1+player] = 0\n",
    "                self._state[0, 0] += 1\n",
    "                self._state[0, 1] = player\n",
    "            if bid == 2:                        # Increment on Item_2, stay on Item_1\n",
    "                self._state[0,1+player] = 0\n",
    "                self._state[1,1+player] = 1\n",
    "                self._state[1, 0] += 1\n",
    "                self._state[1, 1] = player\n",
    "            if bid == 3:                        # Increment on both\n",
    "                self._state[0, 0] += 1\n",
    "                self._state[1, 0] += 1\n",
    "                self._state[0, 1] = player\n",
    "                self._state[1, 1] = player\n",
    "                self._state[0,1+player] = 1\n",
    "                self._state[1,1+player] = 1\n",
    "\n",
    "        if np.all((self._state[0:1, 2:3] == 0)):\n",
    "            return TimeStep(StepType.MID,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "        return TimeStep(\n",
    "            StepType.MID,\n",
    "            0,\n",
    "            DISCOUNT,\n",
    "            self._state\n",
    "        )\n",
    "    def observation_spec(self) -> types.NestedArraySpec:\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self) -> types.NestedArraySpec:\n",
    "        return self._action_spec\n",
    "\n",
    "    def get_info(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    def get_state(self) -> Any:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, state: Any) -> None:\n",
    "        self._state = state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "class IMAgent(DqnAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: TFPyEnvironment,\n",
    "                 observation_spec: TensorSpec = None,\n",
    "                 action_spec: TensorSpec = None,\n",
    "                 reward_fn: Callable = lambda time_step: time_step.reward,\n",
    "                 action_fn: Callable = lambda action: action,\n",
    "                 name: str='IMAgent',\n",
    "                 q_network=None,\n",
    "                 # training params\n",
    "                 replay_buffer_max_length: int = 1000,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 training_batch_size: int = 8,\n",
    "                 training_parallel_calls: int = 3,\n",
    "                 training_prefetch_buffer_size: int = 3,\n",
    "                 training_num_steps: int = 2,\n",
    "                 **dqn_kwargs):\n",
    "\n",
    "        self._env = env\n",
    "        self._reward_fn = reward_fn\n",
    "        self._name = name\n",
    "        self._observation_spec = observation_spec or self._env.observation_spec()\n",
    "        self._action_spec = action_spec or self._env.action_spec()\n",
    "        self._action_fn = action_fn\n",
    "\n",
    "        q_network = q_network or self._build_q_net()\n",
    "\n",
    "        env_ts_spec = self._env.time_step_spec()\n",
    "        time_step_spec = TimeStep(\n",
    "            step_type=env_ts_spec.step_type,\n",
    "            reward=env_ts_spec.reward,\n",
    "            discount=env_ts_spec.discount,\n",
    "            observation=q_network.input_tensor_spec\n",
    "        )\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        super().__init__(time_step_spec,\n",
    "                         self._action_spec,\n",
    "                         q_network,\n",
    "                         optimizer,\n",
    "                         name=name,\n",
    "                         **dqn_kwargs)\n",
    "\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size)\n",
    "        self._rewards = []\n",
    "\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=self.collect_data_spec,\n",
    "            batch_size=self._env.batch_size,\n",
    "            max_length=replay_buffer_max_length)\n",
    "\n",
    "        self._training_batch_size = training_batch_size\n",
    "        self._training_parallel_calls = training_parallel_calls\n",
    "        self._training_prefetch_buffer_size = training_prefetch_buffer_size\n",
    "        self._training_num_steps = training_num_steps\n",
    "        self.train = common.function(self.train)\n",
    "\n",
    "    def _build_q_net(self):\n",
    "#         q_net = QRnnNetwork(input_tensor_spec=self._observation_spec,\n",
    "#                             action_spec=self._action_spec,\n",
    "#                             name=f'{self._name}QRNN')\n",
    "\n",
    "        fc_layer_params = (50,)\n",
    "        q_net = QNetwork(\n",
    "            self._observation_spec,\n",
    "            self._action_spec,\n",
    "            fc_layer_params=fc_layer_params)\n",
    "\n",
    "        q_net.create_variables()\n",
    "        q_net.summary()\n",
    "\n",
    "        return q_net\n",
    "\n",
    "    def reset(self):\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size\n",
    "        )\n",
    "        self._rewards = []\n",
    "\n",
    "    def episode_return(self) -> float:\n",
    "        return np.sum(self._rewards)\n",
    "\n",
    "    def _observation_fn(self, observation: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "            Takes a tensor with specification self._env.observation_spec\n",
    "            and extracts a tensor with specification self._observation_spec.\n",
    "\n",
    "            For example, consider an agent within an NxN maze environment.\n",
    "            The env could expose the entire NxN integer matrix as an observation\n",
    "            but we would prefer the agent to only see a 3x3 window around their\n",
    "            current location. To do this we can override this method.\n",
    "\n",
    "            This allows us to have different agents acting in the same environment\n",
    "            with different observations.\n",
    "        \"\"\"\n",
    "        return observation\n",
    "\n",
    "    def _augment_time_step(self, time_step: TimeStep) -> TimeStep:\n",
    "\n",
    "        reward = self._reward_fn(time_step)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        if reward.shape != time_step.reward.shape:\n",
    "            reward = tf.reshape(reward, time_step.reward.shape)\n",
    "\n",
    "        observation = self._observation_fn(time_step.observation)\n",
    "\n",
    "        return TimeStep(\n",
    "            step_type=time_step.step_type,\n",
    "            reward=reward,\n",
    "            discount=time_step.discount,\n",
    "            observation=observation\n",
    "        )\n",
    "\n",
    "    def _current_time_step(self) -> TimeStep:\n",
    "        time_step = self._env.current_time_step()\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def _step_environment(self, action) -> TimeStep:\n",
    "        action = self._action_fn(action)\n",
    "        time_step = self._env.step(action)\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def act(self, collect=False) -> Trajectory:\n",
    "        time_step = self._current_time_step()\n",
    "\n",
    "        if collect:\n",
    "            policy_step = self.collect_policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "        else:\n",
    "            policy_step = self.policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "\n",
    "        self._policy_state = policy_step.state\n",
    "        next_time_step = self._step_environment(policy_step.action)\n",
    "        traj = trajectory.from_transition(time_step, policy_step, next_time_step)\n",
    "\n",
    "        self._rewards.append(next_time_step.reward)\n",
    "\n",
    "        if collect:\n",
    "            self._replay_buffer.add_batch(traj)\n",
    "\n",
    "        return traj\n",
    "\n",
    "    def train_iteration(self) -> LossInfo:\n",
    "        experience, info = self._replay_buffer.get_next(\n",
    "            sample_batch_size=self._training_batch_size,\n",
    "            num_steps=self._training_num_steps\n",
    "        )\n",
    "        return self.train(experience)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  550       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             multiple                  255       \n",
      "=================================================================\n",
      "Total params: 805\n",
      "Trainable params: 805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  550       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             multiple                  255       \n",
      "=================================================================\n",
      "Total params: 805\n",
      "Trainable params: 805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def ttt_action_fn(player, action):\n",
    "    return {'position': action, 'value': player}\n",
    "valuation = np.random.rand(2,2) * 1000\n",
    "auction = EnglishAuction(valuation)\n",
    "auction = tf_py_environment.TFPyEnvironment(auction)\n",
    "player_1 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,1),\n",
    "                   name=\"player1\"\n",
    "                   )\n",
    "\n",
    "player_2 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,2),\n",
    "                   name=\"player2\"\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}