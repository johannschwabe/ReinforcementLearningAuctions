{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/16211939622767980\n"
     ]
    }
   ],
   "source": [
    "# Code based of https://dylancope.github.io/Multiagent-RL-with-TFAgents/\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "import random\n",
    "from time import time\n",
    "from typing import Tuple, List, Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_agents.typing import types\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents import DqnAgent\n",
    "from tf_agents.agents.tf_agent import LossInfo\n",
    "from tf_agents.environments.py_environment import PyEnvironment\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "from tf_agents.specs import TensorSpec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories.time_step import TimeStep\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n\\n')\n",
    "\n",
    "OUTPUTS_DIR = f'./outputs/{int(10000000 * time())}'\n",
    "print('Output Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "LOST = 0\n",
    "DISCOUNT = np.float32(1.0)\n",
    "import abc\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import BoundedArraySpec\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories.time_step import StepType\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "class EnglishAuction(py_environment.PyEnvironment):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        bid_spec = BoundedArraySpec(\n",
    "            (), dtype=np.int32, minimum=0, maximum=4\n",
    "        )\n",
    "        player_spec = BoundedArraySpec(shape=(1,),dtype=np.int32, minimum=1, maximum=1)\n",
    "        self._action_spec = {\n",
    "            \"bid\": bid_spec,\n",
    "            \"player\": player_spec\n",
    "        }\n",
    "        minimum = np.zeros((2,5), dtype=np.int32)\n",
    "        maximum = np.array([[1000,2,1,1,1000],[1000,2,1,1,1000]], dtype=np.int32)\n",
    "        self._observation_spec = BoundedArraySpec(\n",
    "            shape=(2,5), dtype=np.int32, name=\"observation\", maximum=maximum, minimum=minimum\n",
    "        )\n",
    "        # highest bid, winning player, p1 active, p2 active, p1 valuation, p2 valuation\n",
    "        self._state = np.zeros(shape=(2,6), dtype=np.int32)\n",
    "        self._state[:, 2:4] = 1\n",
    "        valuations = np.random.rand(2,2) * 1000\n",
    "        self._state[:,4:6] = valuations\n",
    "        self._done = False\n",
    "        self._current_time_step = TimeStep(step_type=StepType.FIRST, reward=0, discount=DISCOUNT, observation=self._state)\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = np.zeros(shape=(2,6), dtype=np.int32)\n",
    "        self._done = False\n",
    "        valuations = np.random.rand(2,2) * 1000\n",
    "        self._state[:,4:6] = valuations\n",
    "        self._state[:,2:4] = 1\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def __calculateWinnings(self, player):\n",
    "        item_1 = (self._state[0,1] == player) * (self._state[0, 3 + player] - self._state[0,0])\n",
    "        item_2 = (self._state[1,1] == player) * (self._state[1, 3 + player] - self._state[1,0])\n",
    "        return np.float32(item_1 + item_2)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._done:\n",
    "            return self._reset()\n",
    "\n",
    "        player = action[\"player\"]\n",
    "        bid = action[\"bid\"]\n",
    "\n",
    "        if np.all((self._state[0:1, 2:3] == 0)):\n",
    "            self._done = True\n",
    "            return TimeStep(StepType.LAST,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "\n",
    "        if bid == 0:                        # Pass both\n",
    "            self._state[0,1+player] = 0\n",
    "            self._state[1,1+player] = 0\n",
    "        if bid == 1:                        # Increment on Item_1, stay on Item_2\n",
    "            self._state[0,1+player] = 1\n",
    "            self._state[1,1+player] = 0\n",
    "            self._state[0, 0] += 1\n",
    "            self._state[0, 1] = player\n",
    "        if bid == 2:                        # Increment on Item_2, stay on Item_1\n",
    "            self._state[0,1+player] = 0\n",
    "            self._state[1,1+player] = 1\n",
    "            self._state[1, 0] += 1\n",
    "            self._state[1, 1] = player\n",
    "        if bid == 3:                        # Increment on both\n",
    "            self._state[0, 0] += 1\n",
    "            self._state[1, 0] += 1\n",
    "            self._state[0, 1] = player\n",
    "            self._state[1, 1] = player\n",
    "            self._state[0,1+player] = 1\n",
    "            self._state[1,1+player] = 1\n",
    "\n",
    "        if np.all((self._state[0:1, 2:3] == 0)):\n",
    "            return TimeStep(StepType.MID,\n",
    "                            self.__calculateWinnings(player),\n",
    "                            DISCOUNT,\n",
    "                            self._state\n",
    "                            )\n",
    "        return TimeStep(\n",
    "            StepType.MID,\n",
    "            np.float32(0),\n",
    "            DISCOUNT,\n",
    "            self._state\n",
    "        )\n",
    "    def observation_spec(self) -> types.NestedArraySpec:\n",
    "        return self._observation_spec\n",
    "\n",
    "    def action_spec(self) -> types.NestedArraySpec:\n",
    "        return self._action_spec\n",
    "\n",
    "    def get_info(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    def get_state(self) -> Any:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, state: Any) -> None:\n",
    "        self._state = state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class IMAgent(DqnAgent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 env: TFPyEnvironment,\n",
    "                 player_id,\n",
    "                 observation_spec: TensorSpec = None,\n",
    "                 action_spec: TensorSpec = None,\n",
    "                 reward_fn: Callable = lambda time_step: time_step.reward,\n",
    "                 action_fn: Callable = lambda action: action,\n",
    "                 name: str='IMAgent',\n",
    "                 q_network=None,\n",
    "                 # training params\n",
    "                 replay_buffer_max_length: int = 1000,\n",
    "                 learning_rate: float = 1e-5,\n",
    "                 training_batch_size: int = 8,\n",
    "                 training_parallel_calls: int = 3,\n",
    "                 training_prefetch_buffer_size: int = 3,\n",
    "                 training_num_steps: int = 2,\n",
    "                 **dqn_kwargs):\n",
    "\n",
    "        self._env = env\n",
    "        self._reward_fn = reward_fn\n",
    "        self._name = name\n",
    "        self._observation_spec = observation_spec or self._env.observation_spec()\n",
    "        self._action_spec = action_spec or self._env.action_spec()\n",
    "        self._action_fn = action_fn\n",
    "        self.player_id = player_id\n",
    "\n",
    "        q_network = q_network or self._build_q_net()\n",
    "\n",
    "        env_ts_spec = self._env.time_step_spec()\n",
    "        time_step_spec = TimeStep(\n",
    "            step_type=env_ts_spec.step_type,\n",
    "            reward=env_ts_spec.reward,\n",
    "            discount=env_ts_spec.discount,\n",
    "            observation=q_network.input_tensor_spec\n",
    "        )\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        super().__init__(time_step_spec,\n",
    "                         self._action_spec,\n",
    "                         q_network,\n",
    "                         optimizer,\n",
    "                         name=name,\n",
    "                         **dqn_kwargs)\n",
    "\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size)\n",
    "        self._rewards = []\n",
    "\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=self.collect_data_spec,\n",
    "            batch_size=self._env.batch_size,\n",
    "            max_length=replay_buffer_max_length)\n",
    "\n",
    "        self._training_batch_size = training_batch_size\n",
    "        self._training_parallel_calls = training_parallel_calls\n",
    "        self._training_prefetch_buffer_size = training_prefetch_buffer_size\n",
    "        self._training_num_steps = training_num_steps\n",
    "        self.train = common.function(self.train)\n",
    "\n",
    "    def _build_q_net(self):\n",
    "#         q_net = QRnnNetwork(input_tensor_spec=self._observation_spec,\n",
    "#                             action_spec=self._action_spec,\n",
    "#                             name=f'{self._name}QRNN')\n",
    "\n",
    "        fc_layer_params = (75, 40)\n",
    "        q_net = QNetwork(\n",
    "            self._observation_spec,\n",
    "            self._action_spec,\n",
    "            fc_layer_params=fc_layer_params)\n",
    "\n",
    "        q_net.create_variables()\n",
    "        q_net.summary()\n",
    "\n",
    "        return q_net\n",
    "\n",
    "    def reset(self):\n",
    "        self._policy_state = self.policy.get_initial_state(\n",
    "            batch_size=self._env.batch_size\n",
    "        )\n",
    "        self._rewards = []\n",
    "\n",
    "    def episode_return(self) -> float:\n",
    "        return np.sum(self._rewards)\n",
    "\n",
    "    def _observation_fn(self, observation: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "            Takes a tensor with specification self._env.observation_spec\n",
    "            and extracts a tensor with specification self._observation_spec.\n",
    "\n",
    "            For example, consider an agent within an NxN maze environment.\n",
    "            The env could expose the entire NxN integer matrix as an observation\n",
    "            but we would prefer the agent to only see a 3x3 window around their\n",
    "            current location. To do this we can override this method.\n",
    "\n",
    "            This allows us to have different agents acting in the same environment\n",
    "            with different observations.\n",
    "        \"\"\"\n",
    "        correct_column_index = 3 + self.player_id\n",
    "        shared = tf.slice(observation, [0,0,0], [1,2,4])\n",
    "        correct_column = observation[:,:,correct_column_index:correct_column_index+1]\n",
    "        res = tf.concat([shared, correct_column],2)\n",
    "        return res\n",
    "\n",
    "    def _augment_time_step(self, time_step: TimeStep) -> TimeStep:\n",
    "\n",
    "        reward = self._reward_fn(time_step)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        if reward.shape != time_step.reward.shape:\n",
    "            reward = tf.reshape(reward, time_step.reward.shape)\n",
    "\n",
    "        observation = self._observation_fn(time_step.observation)\n",
    "\n",
    "        return TimeStep(\n",
    "            step_type=time_step.step_type,\n",
    "            reward=reward,\n",
    "            discount=time_step.discount,\n",
    "            observation=observation\n",
    "        )\n",
    "\n",
    "    def _current_time_step(self) -> TimeStep:\n",
    "        time_step = self._env.current_time_step()\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def _step_environment(self, action) -> TimeStep:\n",
    "        action = self._action_fn(action)\n",
    "        time_step = self._env.step(action)\n",
    "        time_step = self._augment_time_step(time_step)\n",
    "        return time_step\n",
    "\n",
    "    def act(self, collect=False) -> Trajectory:\n",
    "        time_step = self._current_time_step()\n",
    "\n",
    "        if collect:\n",
    "            policy_step = self.collect_policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "        else:\n",
    "            policy_step = self.policy.action(\n",
    "                time_step, policy_state=self._policy_state)\n",
    "\n",
    "        self._policy_state = policy_step.state\n",
    "        next_time_step = self._step_environment(policy_step.action)\n",
    "        traj = trajectory.from_transition(time_step, policy_step, next_time_step)\n",
    "\n",
    "        self._rewards.append(next_time_step.reward)\n",
    "\n",
    "        if collect:\n",
    "            self._replay_buffer.add_batch(traj)\n",
    "\n",
    "        return traj\n",
    "\n",
    "    def train_iteration(self) -> LossInfo:\n",
    "        experience, info = self._replay_buffer.get_next(\n",
    "            sample_batch_size=self._training_batch_size,\n",
    "            num_steps=self._training_num_steps\n",
    "        )\n",
    "        return self.train(experience)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "def training_episode(env, player_nr_1, player_nr_2):\n",
    "    time_step = env.reset()\n",
    "    player_nr_1.reset()\n",
    "    player_nr_2.reset()\n",
    "    time_steps = []\n",
    "    players = cycle([player_nr_1, player_nr_2])\n",
    "    while not time_step.is_last():\n",
    "        player = next(players)\n",
    "        player.act(collect=True)\n",
    "        time_step = env.current_time_step()\n",
    "        time_steps.append(time_step)\n",
    "    return time_steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def collect_training_data():\n",
    "    for game in range(episodes_per_iteration):\n",
    "        training_episode(auction, player_1, player_2)\n",
    "\n",
    "        p1_return = player_1.episode_return()\n",
    "        p2_return = player_2.episode_return()\n",
    "\n",
    "        outcome = \"draw\"\n",
    "        if p1_return > p2_return:\n",
    "            outcome = \"p1_win\"\n",
    "        elif p1_return < p2_return:\n",
    "            outcome = \"p2_win\"\n",
    "        games.append({\n",
    "            'iteration': iteration,\n",
    "            'game': game,\n",
    "            'p1_return': p1_return,\n",
    "            'p2_return': p2_return,\n",
    "            'outcome': outcome,\n",
    "            'final_step': auction.current_time_step()\n",
    "\n",
    "        })\n",
    "\n",
    "def train():\n",
    "    for _ in range (train_steps_per_iteration):\n",
    "        p1_train_info = player_1.train_iteration()\n",
    "        p2_train_info = player_2.train_iteration()\n",
    "\n",
    "        loss_infos.append({\n",
    "            'iteration': iteration,\n",
    "            'p1_loss': p1_train_info.loss.numpy(),\n",
    "            'p2_loss': p2_train_info.loss.numpy(),\n",
    "        })\n",
    "\n",
    "def plot_history():\n",
    "\n",
    "    games_data = pd.DataFrame.from_records(games)\n",
    "    loss_data = pd.DataFrame.from_records(loss_infos)\n",
    "    loss_data['Player 1'] = np.log(loss_data.p1_loss)\n",
    "    loss_data['Player 2'] = np.log(loss_data.p2_loss)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    loss_melted = pd.melt(loss_data,\n",
    "                          id_vars=['iteration'],\n",
    "                          value_vars=['Player 1', 'Player 2'])\n",
    "    smoothing = iteration // 50\n",
    "    loss_melted.iteration = smoothing * (loss_melted.iteration // smoothing)\n",
    "\n",
    "    sns.lineplot(ax=axs[0][0],\n",
    "                 x='iteration', hue='variable',\n",
    "                 y='value', data=loss_melted)\n",
    "    axs[0][0].set_title('Loss History')\n",
    "    axs[0][0].set_ylabel('log-loss')\n",
    "\n",
    "    returns_melted = pd.melt(games_data,\n",
    "                             id_vars=['iteration'],\n",
    "                             value_vars=['p1_return', 'p2_return'])\n",
    "    returns_melted.iteration = smoothing * (returns_melted.iteration // smoothing)\n",
    "    sns.lineplot(ax=axs[0][1],\n",
    "                 x='iteration', hue='variable',\n",
    "                 y='value', data=returns_melted)\n",
    "    axs[0][1].set_title('Return History')\n",
    "    axs[0][1].set_ylabel('return')\n",
    "\n",
    "    games_data['p1_win'] = games_data.outcome == 'p1_win'\n",
    "    games_data['p2_win'] = games_data.outcome == 'p2_win'\n",
    "    games_data['illegal'] = games_data.outcome == 'illegal'\n",
    "    grouped_games_data = games_data.groupby('iteration')\n",
    "    cols = ['game', 'p1_win', 'p2_win', 'illegal']\n",
    "    grouped_games_data = grouped_games_data[cols]\n",
    "    game_totals =  grouped_games_data.max()['game'] + 1\n",
    "    summed_games_data = grouped_games_data.sum()\n",
    "    summed_games_data['p1_win_rate'] = summed_games_data.p1_win / game_totals\n",
    "    summed_games_data['p2_win_rate'] = summed_games_data.p2_win / game_totals\n",
    "    summed_games_data['illegal_rate'] = summed_games_data.illegal / game_totals\n",
    "    summed_games_data['iteration'] = smoothing * (summed_games_data.index // smoothing)\n",
    "\n",
    "    sns.lineplot(ax=axs[1][0],\n",
    "                 x='iteration',\n",
    "                 y='p1_win_rate',\n",
    "                 data=summed_games_data,\n",
    "                 label='Player 1 Win Rate')\n",
    "    sns.lineplot(ax=axs[1][0],\n",
    "                 x='iteration',\n",
    "                 y='p2_win_rate',\n",
    "                 data=summed_games_data,\n",
    "                 label='Player 2 Win Rate')\n",
    "    sns.lineplot(ax=axs[1][0],\n",
    "                 x='iteration',\n",
    "                 y='illegal_rate',\n",
    "                 data=summed_games_data,\n",
    "                 label='Illegal Ending Rate')\n",
    "    axs[1][0].set_title('Outcomes History')\n",
    "    axs[1][0].set_ylabel('ratio')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "num_iterations = 1\n",
    "initial_collect_episodes = 50\n",
    "episodes_per_iteration = 50\n",
    "train_steps_per_iteration = 1\n",
    "training_batch_size = 512\n",
    "training_num_steps = 2\n",
    "replay_buffer_size = 3 * episodes_per_iteration * 9\n",
    "learning_rate = 1e-2\n",
    "plot_interval = 5\n",
    "\n",
    "iteration = 1\n",
    "games = []\n",
    "loss_infos = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  3865      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  205       \n",
      "=================================================================\n",
      "Total params: 4,070\n",
      "Trainable params: 4,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"QNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EncodingNetwork (EncodingNet multiple                  3865      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             multiple                  205       \n",
      "=================================================================\n",
      "Total params: 4,070\n",
      "Trainable params: 4,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def ttt_action_fn(player, action):\n",
    "    return {'position': action, 'value': player}\n",
    "\n",
    "auction = EnglishAuction()\n",
    "auction = tf_py_environment.TFPyEnvironment(auction)\n",
    "player_1 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,1),\n",
    "                   name=\"player1\",\n",
    "                   learning_rate=learning_rate,\n",
    "                   training_batch_size=training_batch_size,\n",
    "                   training_num_steps=training_num_steps,\n",
    "                   replay_buffer_max_length=replay_buffer_size,\n",
    "                   td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                   player_id=1\n",
    "                   )\n",
    "\n",
    "player_2 = IMAgent(env=auction,\n",
    "                   action_spec=auction.action_spec()[\"bid\"],\n",
    "                   action_fn=partial(ttt_action_fn,2),\n",
    "                   name=\"player2\",\n",
    "                   learning_rate=learning_rate,\n",
    "                   training_batch_size=training_batch_size,\n",
    "                   training_num_steps=training_num_steps,\n",
    "                   replay_buffer_max_length=replay_buffer_size,\n",
    "                   td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                   player_id=2\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Initial Training Sample...\n",
      "Samples collected\n"
     ]
    }
   ],
   "source": [
    "print('Collecting Initial Training Sample...')\n",
    "for _ in range(initial_collect_episodes):\n",
    "    training_episode(auction, player_1, player_2)\n",
    "print('Samples collected')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "try:\n",
    "    if iteration > 1:\n",
    "        plot_history()\n",
    "        clear_output(wait=True)\n",
    "    while iteration < num_iterations:\n",
    "        collect_training_data()\n",
    "        train()\n",
    "        iteration += 1\n",
    "        if iteration % plot_interval == 0:\n",
    "            plot_history()\n",
    "            clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    clear_output(wait=True)\n",
    "    print('Interrupting training, plotting history...')\n",
    "    plot_history()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "Item 1: p1: 330, p2: 934\n",
      "Item 2: p1: 126, p2: 747\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "eval is not supported when eager execution is enabled, is .numpy() what you're looking for?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-cd983afd8879>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mtraj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0mts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplayer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_current_time_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mact\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtraj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     print(f'Player: {player.name}, '\n\u001B[1;32m     14\u001B[0m           \u001B[0;34mf'Action: {(act % 3, act // 3)}, '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/git/MML/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36meval\u001B[0;34m(self, feed_dict, session)\u001B[0m\n\u001B[1;32m   1255\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1257\u001B[0;31m     raise NotImplementedError(\n\u001B[0m\u001B[1;32m   1258\u001B[0m         \u001B[0;34m\"eval is not supported when eager execution is enabled, \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \"is .numpy() what you're looking for?\")\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: eval is not supported when eager execution is enabled, is .numpy() what you're looking for?"
     ]
    }
   ],
   "source": [
    "ts = auction.reset()\n",
    "player_1.reset()\n",
    "player_2.reset()\n",
    "print('Start:')\n",
    "print(f'Item 1: p1: {ts.observation.numpy()[0,0,4]}, p2: {ts.observation.numpy()[0,0,5]}')\n",
    "print(f'Item 2: p1: {ts.observation.numpy()[0,1,4]}, p2: {ts.observation.numpy()[0,1,5]}')\n",
    "players = cycle([player_1, player_2])\n",
    "while not ts.is_last():\n",
    "    player = next(players)\n",
    "    traj = player.act()\n",
    "    ts = player._current_time_step()\n",
    "    act = traj.action[0].eval()\n",
    "    print(f'Player: {player.name}, '\n",
    "          f'Action: {(act % 3, act // 3)}, '\n",
    "          f'Reward: {ts.reward.numpy()[0]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}